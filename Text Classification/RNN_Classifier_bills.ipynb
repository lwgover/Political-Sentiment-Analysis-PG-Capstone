{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vaguely following the tutorial: \n",
    "https://www.tensorflow.org/text/tutorials/text_classification_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses.ascii import isalpha, isdigit\n",
    "from plistlib import InvalidFileException\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from Bill import Bill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bills(folder:str):\n",
    "    bill_locations = os.listdir(folder)\n",
    "    bill_locations = list(map(lambda loc:folder + '/' + loc, bill_locations))\n",
    "    bills = []\n",
    "    for bill_file in bill_locations:\n",
    "        if bill_file[-3:] == 'txt':\n",
    "            try:\n",
    "                bills.append(Bill(bill_file))\n",
    "            except InvalidFileException as e:\n",
    "                continue\n",
    "    return bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "GloVe_file = '/Users/lucasgover/Downloads/glove.6B/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emmbed_dict = {}\n",
    "with open(GloVe_file,'r') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:],'float32')\n",
    "    emmbed_dict[word]=vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(list(emmbed_dict.values())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26809   0.19536  -0.85239   0.17078   0.49344   0.17885  -1.1485\n",
      " -1.5259    0.32724  -0.41434   0.32616   1.0239    0.55419   0.3874\n",
      "  1.0458    0.31049   0.067686 -0.14538  -1.2247    0.8357    0.067415\n",
      "  0.45555   0.41626   0.30002  -0.2766    0.28402  -0.78093  -0.36034\n",
      " -0.6686   -0.40467   0.33729  -0.39184   0.1391    1.0417   -0.89186\n",
      " -0.28363  -0.63136  -1.0775    0.18568  -0.98521   0.85771  -0.25934\n",
      "  0.25918   0.64003  -0.27659   0.45614   0.56947  -0.36858  -0.84004\n",
      " -0.26615 ]\n",
      "[ 0.24527    0.62001   -0.24089    0.07491    0.0024258  0.53981\n",
      " -1.6029     0.21598   -1.3212    -0.098482   0.73315    0.7586\n",
      "  0.19109    1.0154     0.74009    0.068095   0.67282   -0.95646\n",
      " -2.1396     0.69536   -1.3298    -1.0708     0.081902  -0.43348\n",
      " -0.58345   -0.24443    0.30174   -0.040186   0.46538   -1.1714\n",
      "  0.386      1.5198     0.091624   0.068236  -0.73884    0.21032\n",
      " -0.36036    0.22627    0.27641   -0.32114   -0.17974   -0.11887\n",
      "  0.38149    0.33808   -0.16036    0.46927   -0.17116    0.21776\n",
      " -0.79608   -0.12026  ]\n"
     ]
    }
   ],
   "source": [
    "print(emmbed_dict['penguin'])\n",
    "print(emmbed_dict['penguins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Bills Here\n",
    "BillsLocation = '/Users/lucasgover/Desktop/AI431_Projects/AIFinal/Bills_2021-2022'\n",
    "bills = make_bills(BillsLocation)\n",
    "\n",
    "\n",
    "#Turn into tf datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(bill:Bill):\n",
    "    words = []\n",
    "    current_word = ''\n",
    "    for character in bill.text.lower():\n",
    "        if isdigit(character) or isalpha(character):\n",
    "            current_word += character\n",
    "        else:\n",
    "            if len(current_word) > 0:\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_embedding_dataset(words:list):\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if(word in emmbed_dict):\n",
    "            emmbeddings += emmbed_dict[word]\n",
    "        else:\n",
    "            # Do something else?\n",
    "            print(word + \" was not in the embedding dict!\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bills_to_dataset(bills_list:list):\n",
    "    CHUNK_SIZE = 100\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for bill in bills_list:\n",
    "        words = get_words(bill)\n",
    "        num_chunks = len(words) // CHUNK_SIZE\n",
    "        chunks = []\n",
    "        while(len(words) > CHUNK_SIZE):\n",
    "            chunks += [dataset_to_embedding_dataset(words[:CHUNK_SIZE])]\n",
    "            words = words[CHUNK_SIZE:]\n",
    "        texts += chunks\n",
    "        if bill.party == 'D' or 'd':\n",
    "            labels += [-1] * num_chunks\n",
    "        elif bill.party == 'R' or 'r':\n",
    "            labels += [1] * num_chunks\n",
    "        elif bills.party == 'I' or 'i':\n",
    "            labels += [-1] * num_chunks\n",
    "        else:\n",
    "            labels += [0] * num_chunks\n",
    "    return texts,labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'builtin_function_or_method' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text Classification/RNN_Classifier_bills.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bill_texts,bill_labels \u001b[39m=\u001b[39m bills_to_dataset(bills)\n",
      "\u001b[1;32m/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text Classification/RNN_Classifier_bills.ipynb Cell 13\u001b[0m in \u001b[0;36mbills_to_dataset\u001b[0;34m(bills_list)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m chunks \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m(\u001b[39mlen\u001b[39m(words) \u001b[39m>\u001b[39m CHUNK_SIZE):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     chunks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [dataset_to_embedding_dataset(words[:CHUNK_SIZE])]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     words \u001b[39m=\u001b[39m words[CHUNK_SIZE:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m texts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunks\n",
      "\u001b[1;32m/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text Classification/RNN_Classifier_bills.ipynb Cell 13\u001b[0m in \u001b[0;36mdataset_to_embedding_dataset\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m(word \u001b[39min\u001b[39;49;00m emmbed_dict\u001b[39m.\u001b[39;49mkeys):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         emmbeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m emmbed_dict[word]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lucasgover/Desktop/Political-Sentiment-Analysis-PG-Capstone/Text%20Classification/RNN_Classifier_bills.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# Do something else?\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'builtin_function_or_method' is not iterable"
     ]
    }
   ],
   "source": [
    "bill_texts,bill_labels = bills_to_dataset(bills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a sample text with padding\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two cells above should output the same thing? I'm not sure what padding is right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
